\documentclass[a4paper,onecolumn]{article}
\usepackage{hyperref}
% for setting the linespace (\setstretch)
%\usepackage{setspace}
% distance between the columns
%\setlength{\columnsep}{1cm}
% for comments
\usepackage{verbatim}
% filling with lipsum text
\usepackage{lipsum}
% ams
\usepackage{amssymb,amsfonts,amsmath,bm}
\usepackage[pdftex]{graphicx}
\bibliographystyle{plain}

\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{mcsg v0.3.1}
\author{
        Simone Marsili \\
        simo.marsili@gmail.com
}
\date{\today}


\begin{document}
\maketitle
%\abstract{\lipsum[78]}

\section{Requirements}
\label{sec:requirements}

In order to compile \verb|mcsg|, you will need a Fortran compiler installed on your machine.   
If you are using Debian or a Debian derivative such as Ubuntu, you can install the \verb|gfortran| compiler using the following command:
\begin{verbatim}
>> sudo apt-get install gfortran
\end{verbatim}

The inference algorithm works by simulating a swarm of persistent Markov chains. 
To compile \verb|mcsg| with support for parallel runs on a distributed-memory architecture,
you will need a valid MPI implementation installed on your machine. 
The code has been tested and is known to work with the latest versions of both OpenMPI and MPICH.   
OpenMPI (recommended) can be installed on Debian derivatives typing:
\begin{verbatim}
sudo apt-get install openmpi-bin libopenmpi1.10 libopenmpi-dev
\end{verbatim}
(For details on running MPI jobs with OpenMPI see \href{https://www.open-mpi.org/faq/?category=running}{this link}).
\\\

The compiling and linking of source files is handled by the Gnu \verb|make|. 
If you are using Debian or a Debian derivative such as Ubuntu, you should find 4.1 already installed.

\section{Compiling}
\label{sec:compiling}
From a Unix terminal, type \verb|make| in the \verb|src| directory:
\begin{verbatim}
>> cd src
>> make
\end{verbatim}
After compiling, you should see these binary files in the \verb|src| directory:\\
\verb|mcsg-learn|, \verb|mcsg-eval| and \verb|mcsg-sample|. \\
To install the binaries mcsg in \verb|/usr/local/bin|, type:
\begin{verbatim}
>> make install
\end{verbatim}
and \verb|make uninstall| to uninstall.\\
The install path can be specified on the make command line as an absolute path,
e.g. :
\begin{verbatim}
>> make install INSTALLDIR=$HOME/.local/bin
\end{verbatim}

\section{mcsg-learn}
\label{sec:mcsg-learn}
Starting from a MSA file in FASTA format, \verb|msa.fa|, the maximum likelihood parameters of a pairwise model for the sequence distribution
can be computed using \verb|mcsg-learn|, that implements the \verb|mcsg| routines for statistical inference.
\begin{verbatim}
>> mcsg-learn --fasta msa.fa --learn-agd 100 --nsweeps 1000 --lambda 0.01
\end{verbatim}
\begin{itemize}
\item \verb|--fasta msa.fa|: read sequences from file \verb|msa.fa|
\item \verb|--learn-agd 100|: perform 100 iterations of accelerated gradient descent of the cost function
\item \verb|--nsweeps 1000|: at each iteration, the gradient of the cost function is estimated from a $1000-$sweeps long MCMC trajectory
\item \verb|--lambda 0.01|: the cost function contains a $l_2$ regularization term: higher values of \verb|--lambda| correspond to more strongly regularized solutions
\end{itemize}


The most simple way to run \verb|mcsg-learn| is to use the default values for the parameters of the run:
\begin{verbatim}
>> mcsg-learn --fasta msa.fa
\end{verbatim}
To increase the accuracy in gradient estimation (without simulating longer Markov chains in sequence space)
\begin{verbatim}
>> mpiexec -n 4 mcsg-learn --nsweeps $NS --fasta msa.fa --learn-agd $NITER --lambda 0.01
\end{verbatim}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{mcsg-eval}
\label{sec:mcsg-eval}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{mcsg-sample}
\label{sec:mcsg-sample}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{System Model}
\label{sec:system-model}

\subsection{Concept}
\lipsum[1]\cite{asimov1951foundation}

\begin{itemize}
	\item \lipsum[53]
	\item \lipsum[11]
\end{itemize}

\lipsum[43]

\subsection{Implementation}
\lipsum[2]

\section{Conclusion}
\label{sec:conclusion}

\lipsum[9-10]

\bibliography{ms}

\end{document}
