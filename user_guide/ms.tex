\documentclass[a4paper,onecolumn]{article}
\usepackage{hyperref}
% for setting the linespace (\setstretch)
%\usepackage{setspace}
% distance between the columns
%\setlength{\columnsep}{1cm}
% for comments
\usepackage{verbatim}
% filling with lipsum text
%\usepackage{lipsum}
% ams
\usepackage{amssymb,amsfonts,amsmath,bm}
\usepackage[pdftex]{graphicx}
\bibliographystyle{unsrt}

\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{
  mcdca v0.3.1
}
\author{
        Simone Marsili \\
        simo.marsili@gmail.com
}
\date{\today}


\begin{document}
\maketitle
%\abstract{}
\verb|mcdca| is a Monte Carlo (MC) code for the analysis and the inference of probabilistic models for sequences of phylogenetically-related proteins, 
within the framework of Direct Coupling Analysis (DCA)\cite{wikipedia2016Direct_coupling_analysis},
a statistical approach to the analysis of multiple sequence alignments of homologous protein (or nucleic acid) sequences.
DCA is based on a statistical model of pairwise interacting residues (pairwise model),
in which the correlations between residues a different positions in the MSA
are explicitly taken into account and explained by ``direct coupling'' parameters\cite{lapedes2002using,weigt2009identification,morcos2011direct}.

Being the probability of an amino acid at a particular position explicitly dependent on the other positions of the alignment,
it is particularly hard to sample from the corresponding distribution in sequence space, and a number of alternative inference strategies have been proposed that rely
on approximations of the target model distribution\cite{burger2010disentangling,morcos2011direct,balakrishnan2011learning,jones2012psicov,ekeberg2013improved}. 
At variance with these approaches, \verb|mcdca| utilizes a ``learn via sampling'' strategy:
the algorithm tries to maximize the original likelihood function for the pairwise model of interacting residues, 
using a stochastic approximation of the likelihood gradient based on Markov chain Monte Carlo simulations (MCMC-based inference, see \verb|mcdca-learn|).

Finally, if the model is a good approximation of the true data generating distribution, then samples from the fitted model should ``resemble`` the observed data with respect to some important test statistics, that identify important aspects of the data\cite{linderman2017using}. 
In an interative approach to modeling of data, starting with once a model has been fitted to the data, the next step should be the evaluation and the 
It has been recently shown\cite{sutto2015residue} that this simple procedure\cite{ackley1985learning,hinton1986learning} (first applied to the analysis of protein sequences by Lapedes {\it et al.}\cite{lapedes2002using}) is succesfull in learning generative models that reproduce the observed correlations between residues,
capturing higher-order data sructures such as subfamilies of protein sequences\cite{sutto2015residue}.



The final model can be used to compare the different stability of protein sequences with respect to the fitted model (\verb|mcdca-eval|). Moreover,
\verb|mcdca| can also be used to simulate a Markov chain of sequences (mimicking the effect of subsequent mutations of a original sequence), sampling from
the ``energy'' landscape learned from a training set of sequences, as proposed in the original paper\cite{sutto2015residue} (see \verb|mcdca-sample|).
\\[8cm]
mcDCA has been developed and is currently mantained by Simone Marsili (simo.marsili@gmail.com).\\
The code is free software and it is released under the 
If you use version v0.3.1 of \verb|mcdca|, please cite this work:\\
Ludovico Sutto, Simone Marsili, Alfonso Valencia, and Francesco~Luigi Gervasio.\\
From residue coevolution to protein conformational ensembles and functional dynamics.\\
{\em Proceedings of the National Academy of Sciences}, 112(44):13567--13572, 2015.

\newpage
\section{Requirements}
\label{sec:requirements}

In order to compile \verb|mcdca|, you will need a Fortran compiler installed on your machine.   
If you are using Debian or a Debian derivative such as Ubuntu, you can install the \verb|gfortran| compiler using the following command:
\begin{verbatim}
>> sudo apt-get install gfortran
\end{verbatim}

The inference algorithm works by simulating a swarm of persistent Markov chains. 
To compile \verb|mcdca| with support for parallel runs,
you will need a valid MPI implementation installed on your machine. 
The code has been tested and is known to work with the latest versions of both OpenMPI and MPICH.   
OpenMPI (recommended) can be installed on Debian derivatives typing:
\begin{verbatim}
sudo apt-get install openmpi-bin libopenmpi1.10 libopenmpi-dev
\end{verbatim}
(For details on running MPI jobs with OpenMPI see \href{https://www.open-mpi.org/faq/?category=running}{this link}).
\\\

The compiling and linking of source files is handled by the Gnu \verb|make|. 
If you are using a Debian derivative, you should find 4.1 already installed.

\newpage
\section{Compiling}
\label{sec:compiling}
From a Unix terminal, type \verb|make| in the \verb|src| directory:
\begin{verbatim}
>> cd src
>> make
\end{verbatim}
After compiling, you should see these binary files in the \verb|src| directory:
\begin{verbatim}
>> ls mcdca-*
mcdca-eval  mcdca-learn  mcdca-sample
\end{verbatim}
To install the \verb|mcdca| binaries in \verb|/usr/local/bin|, type:
\begin{verbatim}
>> make install
\end{verbatim}
and \verb|make uninstall| to uninstall.\\
The install path can be specified on the make command line as an absolute path,
e.g. :
\begin{verbatim}
>> make install INSTALLDIR=$HOME/.local/bin
\end{verbatim}

\newpage
\section{mcdca-learn}
\label{sec:mcdca-learn}
\verb|mcdca-learn| can fit the pairwise model to the sequence data (a MSA in FASTA format). 
More specifically, the parameters of the model will be fitted via maximum a posteriori (MAP) estimation.
The following command will read the sequences from FASTA file \verb|msa.fa| and use default values for the fit: 
\begin{verbatim}
>> mcdca-learn --fasta msa.fa
\end{verbatim}
The MAP inference process is iterative and requires the evaluation of the gradient of a loss function at every step. 
The gradient is estimated by simulating a long Markov chain in sequence space, the larger the number of Monte Carlo steps
(or sweeps, defined as $n_{\rm MC steps} = n_{\rm MC sweeps} \times L$ where $L$ is the number of positions in the alignment) the more accurate the gradient estimate. 
Alternatively, the accuracy of gradient estimations can be improved by simulating multiple parallel (and indepedent) Markov chains: 
\begin{verbatim}
>> mpiexec -n 4 mcdca-learn --fasta msa.fa
\end{verbatim}
In the last command line, four independent chains in sequence space will be simulated. 

Non-default values for the calculation can be passed as additional options (run \verb|mcdca -h| or \verb|mcdca --help| for the full list of options), for example: 
\begin{verbatim}
>> mcdca-learn --fasta msa.fa --learn-agd 100 --nsweeps 1000 --lambda 0.01
\end{verbatim}
where the meaning of the three additional options is: 
\begin{itemize}
\item \verb|--learn-agd 100| (perform 100 iterations of accelerated gradient descent of the cost function)
\item \verb|--nsweeps 1000| (at each iteration, the gradient of the cost function is estimated from a $1000-$sweeps long MCMC trajectory)
\item \verb|--lambda 0.01| (the cost function contains a $l_2$ regularization term: higher values of \verb|--lambda| correspond to more strongly regularized solutions)
\end{itemize}

The output of \verb|mcdca-learn| consists of three files:
\begin{itemize}
\item a binary restart file (\verb|rst|), containing all the info needed to restart the calculation from the final inferred values
\item a parameter file (\verb|prm|), containing the inferred values
\item a logfile (\verb|LEARN.log|)
\end{itemize}
The output filenames can be controlled via the option \verb|--prefix|, that is, \verb|--prefix run0| will dump the three files:
\verb|run0.rst|, \verb|run0.prm| and \verb|run0.log|.

\newpage

\subsection{mcdca-learn: usage}

\begin{verbatim}
mcdca-learn

Usage 
    (single persistent Markov chain)
    mcdca-learn [option]... --fasta <file>

    (n parallel chains):
    mpiexec -n <n> mcdca-learn [option]... --fasta <file>

Required arguments
    --fasta <file>
       multiple sequence alignment in FASTA format

Options 
    (initial parameter guess)
    -p <file>, --prm <file> | -r <file>, --rst <file>
       read initial parameters from custom file of parameters OR from restart file

    (MCMC-based gradient calculation)
    -n <n>, -nsweeps <n>
       set the number of sweeps per gradient estimate (default: 1000)

    (generic options)
    --prefix <string>
       prefix for output files (default: learn.log, learn.rst, learn.prm)
    -h, --help
       display this help and exit
\end{verbatim}

\newpage
\section{mcdca-eval}
\label{sec:mcdca-eval}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\newpage
\section{mcdca-sample}
\label{sec:mcdca-sample}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{System Model}
\label{sec:system-model}

\bibliography{ms}

\end{document}
