\documentclass[a4paper,onecolumn]{article}
\usepackage{hyperref}
% for setting the linespace (\setstretch)
%\usepackage{setspace}
% distance between the columns
%\setlength{\columnsep}{1cm}
% for comments
\usepackage{verbatim}
% filling with lipsum text
%\usepackage{lipsum}
% ams
\usepackage{amssymb,amsfonts,amsmath,bm}
\usepackage[pdftex]{graphicx}
\bibliographystyle{plain}

\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{mcsg v0.3.1}
\author{
        Simone Marsili \\
        simo.marsili@gmail.com
}
\date{\today}


\begin{document}
\maketitle
%\abstract{}

Please cite \cite{sutto2015residue}
\section{Requirements}
\label{sec:requirements}

In order to compile \verb|mcsg|, you will need a Fortran compiler installed on your machine.   
If you are using Debian or a Debian derivative such as Ubuntu, you can install the \verb|gfortran| compiler using the following command:
\begin{verbatim}
>> sudo apt-get install gfortran
\end{verbatim}

The inference algorithm works by simulating a swarm of persistent Markov chains. 
To compile \verb|mcsg| with support for parallel runs,
you will need a valid MPI implementation installed on your machine. 
The code has been tested and is known to work with the latest versions of both OpenMPI and MPICH.   
OpenMPI (recommended) can be installed on Debian derivatives typing:
\begin{verbatim}
sudo apt-get install openmpi-bin libopenmpi1.10 libopenmpi-dev
\end{verbatim}
(For details on running MPI jobs with OpenMPI see \href{https://www.open-mpi.org/faq/?category=running}{this link}).
\\\

The compiling and linking of source files is handled by the Gnu \verb|make|. 
If you are using a Debian derivative, you should find 4.1 already installed.

\section{Compiling}
\label{sec:compiling}
From a Unix terminal, type \verb|make| in the \verb|src| directory:
\begin{verbatim}
>> cd src
>> make
\end{verbatim}
After compiling, you should see these binary files in the \verb|src| directory:
\begin{verbatim}
>> ls mcsg-*
mcsg-eval  mcsg-learn  mcsg-sample
\end{verbatim}
To install the \verb|mcsg| binaries in \verb|/usr/local/bin|, type:
\begin{verbatim}
>> make install
\end{verbatim}
and \verb|make uninstall| to uninstall.\\
The install path can be specified on the make command line as an absolute path,
e.g. :
\begin{verbatim}
>> make install INSTALLDIR=$HOME/.local/bin
\end{verbatim}

\section{Basic usage}
\label{sec:basic}

This section gives an overview of the \verb|mcsg| basic commands.
\subsection{mcsg-learn}
\label{sec:mcsg-learn}
Starting from a MSA file in FASTA format, \verb|mcsg-learn| can be used to fit a statistical model of pairwise interacting positions to sequence data. 
More specifically, the parameters of the model will be fitted via maximum a posteriori (MAP) estimation.
The simplest way to start with \verb|mcsg-learn| is using default values for the parameters tuning the inference process:
\begin{verbatim}
>> mcsg-learn --fasta msa.fa
\end{verbatim}
The latter command will read the sequences from FASTA file \verb|msa.fa| and maximize a $l_2$-regularized likelihodd.
The optimization process is iterative and requires the evaluation of the gradient of a loss function at every step,
that can be estimated by simulating a long Markov chain in sequence space, the larger the number of Monte Carlo steps
(or sweeps, defined as $n_{\rm MC steps} = n_{\rm MC sweeps} \times L$ where $L$ is the number of positions in the alignment) the more accurate the gradient estimate. 
Alternatively, the accuracy of gradient estimations can be improved by simulating multiple parallel (and indepedent) Markov chains: 
\begin{verbatim}
>> mpiexec -n 4 mcsg-learn --fasta msa.fa
\end{verbatim}
In the last command line, four independent chains in sequence space will be simulated. 

Non-default values for the calculation can be passed as additional options (run \verb|mcsg -h| or \verb|mcsg --help| for the full list of options), for example: 
\begin{verbatim}
>> mcsg-learn --fasta msa.fa --learn-agd 100 --nsweeps 1000 --lambda 0.01
\end{verbatim}
where the meaning of the three additional options is: 
\begin{itemize}
\item \verb|--learn-agd 100| (perform 100 iterations of accelerated gradient descent of the cost function)
\item \verb|--nsweeps 1000| (at each iteration, the gradient of the cost function is estimated from a $1000-$sweeps long MCMC trajectory)
\item \verb|--lambda 0.01| (the cost function contains a $l_2$ regularization term: higher values of \verb|--lambda| correspond to more strongly regularized solutions)
\end{itemize}

The output of \verb|mcsg-learn| consists of three files:
\begin{itemize}
\item a binary restart file (\verb|rst|), containing all the info needed to restart the calculation from the final inferred values
\item a parameter file (\verb|prm|), containing the inferred values
\item a logfile (\verb|LEARN.log|)
\end{itemize}
The output filenames can be controlled via the option \verb|--prefix|, that is, \verb|--prefix run0| will dump the three files:
\verb|run0.rst|, \verb|run0.prm| and \verb|run0.log|. 


\section{mcsg-eval}
\label{sec:mcsg-eval}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{mcsg-sample}
\label{sec:mcsg-sample}
From a Unix terminal, type \verb|make| in the \verb|src| directory:

\section{System Model}
\label{sec:system-model}

\bibliography{ms}

\end{document}
